# README #

* Version 1.0

As the era of Dennard scaling comes to an end, both the semiconductor industry and the research community are searching for solutions which allow energy ef􏰂iciency and performance to continue to scale while retaining generality and programmability. Programmable accelerators, which exploit some common characteristic within a domain of applications to achieve ef􏰂iciency gains at the cost of some generality, have emerged as a possible answer. One such characteristic, present in many modern applications, is tolerance to approximation. As a result, approximate accelerators are a new class of accelerators that focus on strategies for improving ef􏰂iciency and maintaining generality at the cost of strict program ac- curacy. In particular, one promising strategy is to "learn" the behavior of a region of code using arti􏰂ficial neural networks which, although inherently imprecise, are highly ef􏰂icient to compute on specialized hardware and can run as a surrogate for the original precise code. In this report, we develop NPiler, an automatic compilation framework for transforming regions of code to a neural-network representation. NPiler consists of three phases: programming, in which the programmer marks candidate regions for the transformation; compilation, in which the compiler selects a topology and trains a suitable neural network; and execution, where the original code is replaced with instructions to invoke the specialized neural accelerator, pass inputs, and retrieve outputs. We have developed NPiler to ef􏰂iciently create and execute an ef􏰂icient neural representation of the code on both on a traditional CPU and in a SIMD envi- ronment such as CUDA or AVX. 